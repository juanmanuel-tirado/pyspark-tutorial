{
 "cells": [
  {
   "cell_type": "raw",
   "id": "25f245d0-4f62-4e1b-afae-11277ced4f2b",
   "metadata": {},
   "source": [
    "# Exported using pretty-jupytter\n",
    "# https://github.com/JanPalasek/pretty-jupyter\n",
    "title: \"PySpark MLlib tutorial \"\n",
    "author: \"Juan M. Tirado\"\n",
    "output:\n",
    "    general:\n",
    "        input: false\n",
    "    html:\n",
    "        toc: true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c39c48-87e6-4ba0-9bad-76129dfa5a7f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook is a collection of examples that illustrate how to use PySpark with MLlib. This whole collection of examples is intended to be a gentle introduction to those interested in the topic, want to have additional examples, or simply are curious about how to start with this library.\n",
    "\n",
    "The following skills are expected to follow these examples:\n",
    "- Python\n",
    "- Understanding Spark structures (Dataframes, RDD)\n",
    "- Basic ML notions. This is not intended to be a ML course although, you can find some theoretical explanations.\n",
    "\n",
    "The examples are designed to work with a simple local environment using the MLlib Dataframe. The MLlib RDD-based API is now in maintenance ([see here](https://spark.apache.org/docs/latest/ml-guide.html#announcement-dataframe-based-api-is-primary-api)). This is why you will see that the main import statement comes from `pyspark.ml` not `pyspark.mllib`.\n",
    "\n",
    "You will need a spark environment to be available in your local path. Refer [here](https://spark.apache.org/docs/latest/api/python/getting_started/install.html) to the official guide for more details. \n",
    "\n",
    "You will need java to be available on your local path. Check it out running `java --version`. If nothing is displayed, check out how to install Java on your machine ([here](https://www.oracle.com/java/technologies/downloads/)). Next, you can easily set up a local environment and install the dependencies used in this notebook.\n",
    "\n",
    "```bash\n",
    "virtualenv env\n",
    "source env/bin/activate\n",
    "pip install urllib3 numpy matplotlib pyspark\n",
    "```\n",
    "\n",
    "Wait until the dependencies are already satisfied. If you are not reading these lines from a Jupyter Notebook :) install and run it.\n",
    "\n",
    "```bash\n",
    "pip install jupyter\n",
    "jupyter-notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88251ce8-8bc1-44f0-a787-c8caee3df44c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data types\n",
    "\n",
    "Spark Dataframes support a collection of rows containing elements with different data types. In the context of ML algorithms, data types such as `boolean`, `string` or even `integer` are not the expected input for most ML algorithms. In this sense, MLlib supports data types such as vectors or matrices. \n",
    "\n",
    "## Dense Vectors\n",
    "\n",
    "A dense vector is an array. PySpark uses `numpy` to run algebraical operations.\n",
    "\n",
    "[Docs](https://spark.apache.org/docs/3.5.0/api/python/reference/api/pyspark.ml.linalg.DenseVector.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34141ebc-3cee-4c77-86c8-2bb33401f556",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum:  [10.0,11.0,12.0,13.0,14.0]\n",
      "Difference:  [-10.0,-9.0,-8.0,-7.0,-6.0]\n",
      "Multiplication:  [0.0,2.0,4.0,6.0,8.0]\n",
      "Division:  [5.0,5.0,5.0,5.0,5.0]\n",
      "Non-zeros:  (array([1, 2, 3, 4]),)\n",
      "Squared distance:  330.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import DenseVector\n",
    "\n",
    "a = DenseVector([0,1,2,3,4])\n",
    "b = DenseVector([10,10,10,10,10])\n",
    "\n",
    "print('Sum: ', a + b)\n",
    "print('Difference: ', a - b)\n",
    "print('Multiplication: ', a * 2)\n",
    "print('Division: ', b / 2)\n",
    "\n",
    "print('Non-zeros: ', a.nonzero())\n",
    "print('Squared distance: ', a.squared_distance(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030d382c-f9e3-4338-9751-7d7f851de0c1",
   "metadata": {},
   "source": [
    "## Sparse Vectors\n",
    "\n",
    "Sparse vectors are designed to represent those vectors where a large number of elements is expected to be zero. These vectors are defined by specifying which positions of the array are different from zero and the assigned values. In \n",
    "\n",
    "```python\n",
    "SparseVector (5 ,[0 ,2 ,4] ,[1 ,3 ,5])\n",
    "```\n",
    "\n",
    "we have five elements with entries 0, 2, and 4 take values 1, 3, and 5.\n",
    "\n",
    "[Docs](https://spark.apache.org/docs/3.5.0/api/python/reference/api/pyspark.ml.linalg.SparseVector.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caca949e-50ba-45c3-855d-a327dfdc1f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse vector:  [1. 0. 3. 0. 5.]\n",
      "Indices:  [0 2 4]\n",
      "Non zeros:  3\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import SparseVector\n",
    "\n",
    "sparse_vector = SparseVector (5 ,[0 ,2 ,4] ,[1 ,3 ,5])\n",
    "print('Sparse vector: ', sparse_vector.toArray ())\n",
    "print('Indices: ', sparse_vector.indices )\n",
    "print('Non zeros: ', sparse_vector.numNonzeros ())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9a1220-0a20-4139-ba5d-ec78a79d3195",
   "metadata": {},
   "source": [
    "# Input/Output\n",
    "\n",
    "We can expect datasets to be available from different storage sources:\n",
    "\n",
    "- Hard disks\n",
    "- HDFS\n",
    "- Databases\n",
    "- Others\n",
    "\n",
    "The `SparkSession` object facilitates the load of data from these sources under different formats (CSV, JSON, text, parquet, databases, etc.). We will show  examples for CSV, libSVM, and images.\n",
    "\n",
    "## CSV\n",
    "\n",
    "Let's assume the following dataset in a CSV format:\n",
    "\n",
    "```csv\n",
    "label,f1,f2,f3,f4\n",
    "0,0,\"one\",2.0,true\n",
    "1,4,\"five\",6.0,false\n",
    "```\n",
    "\n",
    "We instantiate a `SparkSession` object and load the dataset indicating that we have a header and the separation character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3a77c25-16b0-4ff6-bb0b-e18e9226529f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/tmp/dataset.csv.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 21\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Get a session object for our current Spark master\u001b[39;00m\n\u001b[1;32m     16\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExample\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmaster(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[1;32m     18\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcsv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msep\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/tmp/dataset.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m dataset\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# we stop the session\u001b[39;00m\n",
      "File \u001b[0;32m~/code/pyspark-tutorial/env/lib/python3.11/site-packages/pyspark/sql/readwriter.py:307\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n",
      "File \u001b[0;32m~/code/pyspark-tutorial/env/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/code/pyspark-tutorial/env/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/tmp/dataset.csv."
     ]
    }
   ],
   "source": [
    "'''\n",
    "For this example we need the dataset.csv file to be available. \n",
    "\n",
    "Copy and paste the following lines:\n",
    "\n",
    "echo \"\\\n",
    "label,f1,f2,f3,f4\n",
    "0,0,\"one\",2.0,true\n",
    "1,4,\"five\",6.0,false\" > /tmp/dataset.csv\n",
    "\n",
    "'''\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Get a session object for our current Spark master\n",
    "session = SparkSession.builder.appName(\"Example\").master(\"local\").getOrCreate()\n",
    "\n",
    "dataset = session.read.format('csv')\\\n",
    ".option('header', 'true')\\\n",
    ".option('sep', ',')\\\n",
    ".load('/tmp/dataset.csv')\n",
    "\n",
    "dataset.show()\n",
    "\n",
    "# we stop the session\n",
    "session.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafd8271-4ec6-4f6b-ba7c-ad5a40d2c2c8",
   "metadata": {},
   "source": [
    "## libSVM\n",
    "\n",
    "[LibSVM](www.csie.ntu.edu.tw/~cjlin/libsvm/) is a popular format to represent numeric sparse data. \n",
    "\n",
    "The following dataset:\n",
    "\n",
    "```\n",
    "0 128:51  129:159\n",
    "1 130:253 131:159  132:50\n",
    "1 155:48  156:238\n",
    "\n",
    "```\n",
    "\n",
    "Where the first row `0 128:51 129:159` indicates an observation with label 0 and feature $128^{th}$ and $129^{th}$ equal to $51$ and $159$ respectively. We can load this dataset using the `SparkSession` object as we did for the `CSV` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcc4624-f451-4ed8-a463-498899455827",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "For this example we need the dataset.libsvm file to be available. \n",
    "\n",
    "Copy and paste the following lines:\n",
    "\n",
    "echo \"\\\n",
    "0 128:51  129:159\n",
    "1 130:253 131:159  132:50\n",
    "1 155:48  156:238\" > /tmp/dataset.libsvm\n",
    "\n",
    "'''\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "session = SparkSession.builder.appName(\"Example\").master(\"local\").getOrCreate()\n",
    "\n",
    "dataset = session.read.format('libsvm').option('numFeatures',157).load('/tmp/dataset.libsvm')\n",
    "\n",
    "dataset.show()\n",
    "                              \n",
    "# we stop the session\n",
    "session.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610ce25b-ff08-4b85-b872-2fc516680834",
   "metadata": {},
   "source": [
    "## Images\n",
    "\n",
    "LMlib can load images in variety of formats (jpeg, png, etc.). It also supports compressed formats. The resulting DataFrame has a column `image` containing information of the schema.\n",
    "\n",
    "More details in the [docs](https://spark.apache.org/docs/latest/ml-datasource.html#image-data-source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9686d8-15da-4d59-ae1f-e823e30bd508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We download a cat image for later\n",
    "\n",
    "import urllib3\n",
    "import tempfile\n",
    "from IPython.display import Image\n",
    "import sys\n",
    "\n",
    "url='https://unsplash.com/photos/ECfPmkOVZPA/download?ixid=M3wxMjA3fDB8MXxhbGx8fHx8fHx8fHwxNzA2MDA3NDAyfA&force=true&w=640'\n",
    "cat_image = tempfile.gettempdir() + '/kitty.png'\n",
    "\n",
    "http = urllib3.PoolManager()\n",
    "r = http.request('GET', url, preload_content=False)\n",
    "    \n",
    "with open(cat_image, 'wb') as f:\n",
    "    while True:\n",
    "        data = r.read()\n",
    "        if not data:\n",
    "            break\n",
    "        f.write(data)\n",
    "r.release_conn()\n",
    "\n",
    "Image(filename=cat_image)     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3364e28d-8703-44ff-8eb4-ed6e393ffd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "session = SparkSession.builder.appName('Example').master('local').getOrCreate()\n",
    "df = session.read.format('image').option('dropInvalid', True).load(cat_image)\n",
    "\n",
    "df.select('image.origin', 'image.width', 'image.height', 'image.nChannels', 'image.mode').show(truncate=False)\n",
    "\n",
    "# The image data is stored in the image.data column, one image per row.\n",
    "img_data = df.select('image.data').collect()[0]\n",
    "\n",
    "# Do something with img_data...\n",
    "\n",
    "session.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41bb05a-ea6c-4cd2-8f63-a9ef48153a12",
   "metadata": {},
   "source": [
    "# Features\n",
    "\n",
    "One of the main tasks for any data engineer is data preparation. For two reasons:\n",
    "\n",
    "1. Raw data is not ready to be consumed by algorithms\n",
    "2. Preprocessing data is required to improve algorithms performance\n",
    "\n",
    "Incoming data has to be processed in different steps until we reach a successful representation to be consumed by algorithms. MLlib offers a collection of feature-related operations. We can distinguish:\n",
    "\n",
    "- Extraction: extract features from raw data\n",
    "- Transformation: modifying/converting features\n",
    "- Selection: select features based on a certain criteria\n",
    "- [Locality Sensitive Hashing](https://en.wikipedia.org/wiki/Locality-sensitive_hashing) (LSH): algorithms combining feature transformation with other algorithms\n",
    "\n",
    "In general feature processing in MLlib follows these steps:\n",
    "\n",
    "1. **Instantiate** the operator indicating the name of the input and output columns and additional params.\n",
    "2. **Fit the model** invoking the `.fit(...)` method to train a model. Some operators may not require this step if they are not associated with a model.\n",
    "3. **Transform** the input data using the model\n",
    "\n",
    "No need to mention that these steps, the input params, and the input format vary from operator to operator.\n",
    "\n",
    "The following sections present succint examples of different operators.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a61c4f-ae02-4085-96e1-59ba1afffc9f",
   "metadata": {},
   "source": [
    "## Normal standardization\n",
    "\n",
    "Scale a feature to obtain a normal distribution with mean 0 and unit-variance.\n",
    "\n",
    "$x'_i = \\dfrac{x_i-\\mu}{\\sigma}$\n",
    "\n",
    "Where\n",
    "- $x_i$ is the feature value\n",
    "- $\\mu$ is the mean of the distribution and $\\sigma$ is the standard deviation\n",
    "- $x'_i$ becomes the standardized feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cb41bc-dd5e-47ae-b75c-838db904d3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "session = SparkSession.builder.appName('Example').master('local').getOrCreate()\n",
    "\n",
    "values = [[0, Vectors .dense ([1.0,0.1, -1.0])],\n",
    "          [1, Vectors .dense ([2.0 ,1.1 ,1.0])] ,\n",
    "          [2, Vectors .dense ([3.0 ,10.1 ,3.0])]]\n",
    "dataset = session.createDataFrame(values, ['id', 'features'])\n",
    "dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb9ec1c-084d-475f-afd4-3318b59e3ade",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit\n",
    "scaler = StandardScaler(inputCol = 'features', outputCol='standardized', withMean=True, withStd=True)\n",
    "scalerModel = scaler.fit(dataset)\n",
    "\n",
    "# Print some statistics\n",
    "print(\"Mean is: %s with sd: %s\" % (scalerModel.mean, scalerModel.std))\n",
    "\n",
    "# Transform\n",
    "standardized = scalerModel.transform(dataset)\n",
    "standardized.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c8e6e3-589a-4ae6-9c6e-34942d9d5401",
   "metadata": {},
   "source": [
    "We can check if the transformed data has the desired distribution using a `Summarizer` ([docs here](https://spark.apache.org/docs/3.5.0/api/python/reference/api/pyspark.ml.stat.Summarizer.html?highlight=mean#pyspark.ml.stat.Summarizer)). For every feature we have mean set to 0 and standard deviation equal to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc16f852-da01-41e7-9f47-99c0fcc5db5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's see what are the mean and the std now\n",
    "from pyspark.ml.stat import Summarizer\n",
    "summarizer = Summarizer.metrics(\"mean\", \"std\")\n",
    "standardized.select(summarizer.summary(standardized.standardized)).show(truncate=False)\n",
    "\n",
    "session.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d2f70d-9d04-4e89-8851-3bbe8367c115",
   "metadata": {},
   "source": [
    "## Elementwise product\n",
    "\n",
    "This transformer multiplies each input vector by a provided vector, using element-wise multiplication. This operation scales each column by a given scalar ([Hadamard product](https://en.wikipedia.org/wiki/Hadamard_product_%28matrices%29)). For example:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    2 & 3 & 1 \\\\\n",
    "    0 & 8 & -2\n",
    "  \\end{bmatrix} \\circ \\begin{bmatrix}\n",
    "    3 & 1 & 4\n",
    "  \\end{bmatrix} = \\begin{bmatrix}\n",
    "    2 \\times 3 & 3 \\times 1 & 1 \\times 4 \\\\\n",
    "    0 \\times 3 & 8 \\times 1 & -2 \\times 4\n",
    "  \\end{bmatrix} = \\begin{bmatrix}\n",
    "    6 & 3 & 4 \\\\\n",
    "    0 & 8 & -8\n",
    "  \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2de2924-9b0a-428b-ae8d-52e32ad26939",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import ElementwiseProduct\n",
    "\n",
    "session = SparkSession.builder.appName('Example').master('local').getOrCreate()\n",
    "\n",
    "a = [[Vectors.dense ([2,3,1])],\n",
    "     [Vectors.dense ([0,8,-2])]]\n",
    "\n",
    "b = Vectors.dense ([3,1,4])\n",
    "print('b =',b)\n",
    "\n",
    "df_a = session.createDataFrame(a, ['features'])\n",
    "df_a.show()\n",
    "\n",
    "ewp = ElementwiseProduct(inputCol='features', outputCol='product', scalingVec=b)\n",
    "\n",
    "a_b = ewp.transform(df_a)\n",
    "a_b.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9004b2e2-f7d5-47fe-a9bf-6a8b6020b7fe",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "\n",
    "When dealing with many features we can come across the **curse of dimensionality**.\n",
    "- More than three variables are difficult to plot\n",
    "- Performance issues for a large number of features\n",
    "- Features that only add \"noise\" to the problem\n",
    "- Algorithms may find difficult to converge to a solution\n",
    "\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique that aims to find the components that maximize the variance. These are the steps to follow:\n",
    "\n",
    "- Standardize the data\n",
    "- Compute eigenvectors and eigenvalues of the covariance matrix\n",
    "- Sort eigenvalues and pick the $d$ larges values\n",
    "- Construct matrix $\\mathbf{W}$ using the $d$ corresponding eigenvectors\n",
    "- Transform dataset $\\mathbf{X}$ multiplying it by $\\mathbf{W}$\n",
    "\n",
    "The covariance matrix is a symmetric matrix with the covariance between variables. It looks like this\n",
    "\n",
    "$$\n",
    "\\mathrm{cov}(X_i, X_j) = \\mathrm{E}\\begin{bmatrix}\n",
    "(X_i - \\mu_i)(X_j - \\mu_j)\n",
    "\\end{bmatrix} = \\mathrm{E}\\begin{bmatrix}X_iX_j\n",
    "\\end{bmatrix} - \\mu_i\\mu_j\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathrm{cov}(\\mathbf{X},\\mathbf{Y})= \\begin{bmatrix}\n",
    " \\mathrm{cov}(X_1,Y_1) & \\mathrm{cov}(X_1,Y_2) & \\cdots & \\mathrm{cov}(X_1,Y_n) \\\\ \\\\\n",
    " \\mathrm{cov}(X_2,Y_1) & \\mathrm{cov}(X_2,Y_2) & \\cdots & \\mathrm{cov}(X_2,Y_n) \\\\ \\\\\n",
    " \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\\\\n",
    "\\mathrm{cov}(X_n,Y_1) & \\mathrm{cov}(X_n,Y_2) & \\cdots & \\mathrm{cov}(X_n,Y_n) \\\\ \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In MLlib there is a PCA transformer that implements all these steps. By applying the PCA we can obtain a reduced version of the original that maintains most of the relevant information brought by the features. \n",
    "\n",
    "In the example below, we compute the PCA for a dataset of 5 features we wish to convert in a new 3 features dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe73baaf-52cf-41ce-81c2-5701deb135a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "session = SparkSession.builder.appName(\"Example\").master(\"local\").getOrCreate()\n",
    "\n",
    "data = [[Vectors.dense ([1.0 , 0.0, 3.0, 0.0, 7.0])],\n",
    "        [Vectors .dense ([2.0 , 0.0, 3.0, 4.0, 5.0])],\n",
    "        [Vectors .dense ([4.0 , 0.0, 0.0, 6.0, 7.0])]]\n",
    "\n",
    "dataset = session.createDataFrame(data, ['features'])\n",
    "dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0359c1ac-3b60-4ed1-825b-696975d7fdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit PCA\n",
    "pca = PCA(inputCol='features', outputCol='pcaFeatures', k=3)\n",
    "pcaModel = pca.fit(dataset)\n",
    "\n",
    "print(\"The variance for every new feature %s\" % pcaModel.explainedVariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8016057-9381-46ec-b210-528cd60332e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transform the original dataset\n",
    "pcaDataset = pcaModel.transform(dataset)\n",
    "pcaDataset.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8346fe3-2d6d-4f16-9519-56d5393e7580",
   "metadata": {},
   "source": [
    "Observe that the transformed dataset does no longer correspond to any real observation. Any model predictions generated using this transformed data for training, has to be reconstructed. Otherwise, the output will no make any sense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e654ad5e-89eb-45f5-b011-8e13d794e14d",
   "metadata": {},
   "source": [
    "## StringIndexer\n",
    "\n",
    "This is a label indexer that assigns a label to every string in a column. If the value is numeric, first it is casted to string and then indexed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19364d1-08a2-4ffd-a08b-75e68b520e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "session = SparkSession.builder.appName('Example').master('local').getOrCreate()\n",
    "\n",
    "data = session.createDataFrame([['blue'], ['red'],\n",
    "                                ['red'], ['white'],\n",
    "                                ['yellow'], ['red']], ['feature'])\n",
    "data.show()\n",
    "\n",
    "si = StringIndexer(inputCol='feature', outputCol='index', )\n",
    "model = si.fit(data)\n",
    "print('Found labels: ', model.labels)\n",
    "model.transform(data).show()\n",
    "\n",
    "session.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c7ea1b-f403-4b70-9a9c-7ae82587dd00",
   "metadata": {},
   "source": [
    "## One hot encoder\n",
    "\n",
    "This encoder maps a column of indices into a single binary vector. If we have 4 labels, for index 3 we will have $[0,0,0,1,0]$. The output is a `SparseVector`.\n",
    "\n",
    "Observe that the param `dropLast` is `True` by default ignoring the label with index $n-1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d6b0a7-305d-4038-b386-e9101e3b9d01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "session = SparkSession.builder.appName('Example').master('local').getOrCreate()\n",
    "\n",
    "data = session.createDataFrame([['blue'], ['red'],\n",
    "                                ['red'], ['white'],\n",
    "                                ['yellow'], ['red']], ['feature'])\n",
    "data.show()\n",
    "\n",
    "# First we need an input column with indices instead of strings.\n",
    "si = StringIndexer(inputCol='feature', outputCol='indexed')\n",
    "si_model = si.fit(data)\n",
    "indexed = si_model.transform(data)\n",
    "indexed.show()\n",
    "\n",
    "# If we let dropLast=True, the index for yellow will be dropped\n",
    "ohe = OneHotEncoder(inputCol='indexed', outputCol='encoded', dropLast=False)\n",
    "ohe_model = ohe.fit(indexed)\n",
    "ohe_transformed = ohe_model.transform(indexed)\n",
    "\n",
    "'''\n",
    "You can check how setting dropLast=True, the 4th row will be\n",
    "\n",
    "+-------+-------+-------------+\n",
    "|feature|indexed|      encoded|\n",
    "+-------+-------+-------------+\n",
    "| yellow|    3.0|(3,[],[])    |\n",
    "+-------+-------+-------------+\n",
    "'''\n",
    "ohe_transformed.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45bd730-9c4d-433e-91f7-69b02dcaf3b6",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Tokenization is the process of splitting a document into a vector of differentiated tokens. The sentence `\"The quick brown fox jumps over the lazy dog\"` will be split into tokens like in `[\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"]`. Different approaches may split the document using white spaces, commas, regular expressions, or any other character.\n",
    "\n",
    "In MLlib there is a Tokenizer transformer for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b546965-3789-448c-976a-b6037471d436",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark .sql import SparkSession\n",
    "from pyspark .ml. feature import Tokenizer\n",
    "\n",
    "session = SparkSession.builder.appName(\"Example\").master(\"local\").getOrCreate()\n",
    "sentenceDataFrame = session.createDataFrame ([(0, \"Hi I heard about Spark\"),\n",
    "                                            (1, \"I wish Java could use case classes\"),\n",
    "                                            (2, \"Logistic, regression, models, are, neat\")], ['id', 'sentence'])\n",
    "\n",
    "tokenizer = Tokenizer(inputCol='sentence', outputCol='words')\n",
    "tokenizer.transform(sentenceDataFrame).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5820839-7e33-433c-a9b7-6f9ae0ef9da6",
   "metadata": {},
   "source": [
    "## Stop words\n",
    "\n",
    "Natural language is redundant and not every term provides the same amount of information. By stop words we refer to the most common words in a given language. These words are so common that result into non-relevant chunks of information. These words are removed previously to any analysis. There is no a single list of stop words and this changes with every language. \n",
    "\n",
    "MLlib implements the [`StopWordsRemover`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.StopWordsRemover.html#stopwordsremover)  that filters out stop words using a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff192bc6-42de-43a5-a238-095a1775e3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark .sql import SparkSession\n",
    "from pyspark .ml. feature import StopWordsRemover\n",
    "\n",
    "session = SparkSession.builder.appName(\"Example\").master(\"local\").getOrCreate()\n",
    "\n",
    "text = session.createDataFrame ([(0, [\"I\", \"saw\", \"the\", \"red\", \" balloon \"]),\n",
    "                                (1, [\"Mary\", \"had\", \"a\", \" little \", \"lamb\"])\n",
    "                                ], [\"id\", \"raw\"])\n",
    "text.show(truncate=False)\n",
    "\n",
    "remover = StopWordsRemover(inputCol='raw', outputCol='filtered')\n",
    "remover.transform(text).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b565f1ed-cc5c-491a-bf25-7896b96c41d6",
   "metadata": {},
   "source": [
    "## Count Vectorizer\n",
    "\n",
    "This estimator counts the number of occurrences of items in a vocabulary represented in a sparse vector. This is particularly useful to represent a document in terms of the frequency of its elements and it is normally used in probabilistic models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b047fe09-461d-4f68-bfc0-ecbb83ef0318",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark .sql import SparkSession\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "session = SparkSession.builder.appName(\"Example\").master(\"local\").getOrCreate()\n",
    "\n",
    "text = session.createDataFrame ([(0,'yellow red blue'.split()),\n",
    "                                (1, 'red'.split()),\n",
    "                                (2, 'blue white blue'.split()),\n",
    "                                ], [\"id\", \"raw\"])\n",
    "\n",
    "text.show()\n",
    "\n",
    "cv = CountVectorizer(inputCol='raw', outputCol='frequencies')\n",
    "cv_model = cv.fit(text)\n",
    "print('The vocabulary: ',cv_model.vocabulary)\n",
    "frequencies = cv_model.transform(text)\n",
    "frequencies.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86b3579-a951-447e-99bc-d6d0fd7d51a8",
   "metadata": {},
   "source": [
    "## N-grams\n",
    "\n",
    "N-grams are a common input for many algorithms to understand the probability of $n$ words to occur together. The `NGram` transformer outputs a collection of these N-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa4eefe-377d-4f01-8f77-16ca7e6b2721",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark .sql import SparkSession\n",
    "from pyspark.ml.feature import IDF, Tokenizer, NGram\n",
    "\n",
    "session = SparkSession.builder.appName(\"Example\").master(\"local\").getOrCreate()\n",
    "\n",
    "\n",
    "text = session.createDataFrame ([(0, \"Hi I heard about Spark\"),\n",
    "                                (0, \"I wish Java could use case classes \"),\n",
    "                                (0, \"Logistic regression models are neat\")\n",
    "                                ], [\"label\", \"sentence\"])\n",
    "\n",
    "# First we tokenize our dataset\n",
    "tokenizer = Tokenizer(inputCol ='sentence', outputCol ='words')\n",
    "words = tokenizer.transform(text)\n",
    "words.show(truncate=False)\n",
    "\n",
    "# Compute 2-grams\n",
    "ngram = NGram(inputCol='words', outputCol='ngrams', n=2)\n",
    "ngrams = ngram.transform(words)\n",
    "ngrams.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d80441-e3ee-4a05-8133-809171fa8e9f",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "Computing the number of occurrences of a term in a document or a collection of documents we can weight its relevance ([here](http://en.wikipedia.org/wiki/Tf%E2%80%93idf).\n",
    "\n",
    "- **Term-frequency (tf)**: the ratio of occurrences of a term in a document among the total\n",
    "\n",
    "$$\n",
    "tf(t,d) = \\dfrac{f_{t,d}}{\\sum_{t' \\in d} f_{t',d}}\n",
    "$$\n",
    "\n",
    "\n",
    "- **Inverse document frequency (idf)**: measure of how much information the term provides\n",
    "\n",
    "$$\n",
    "idf(t,d) = \\log \\dfrac{|D|}{|d \\in D : t \\in d |} \n",
    "$$\n",
    "\n",
    "Let's assume a corpus of documents $D$ with two documents $d_1$ and $d_2$. With the terms and their frequencies shown below:\n",
    "\n",
    "Document $d_1$\n",
    "\n",
    "|term|frequency|\n",
    "|----|---------|\n",
    "|this|1|\n",
    "|is|1|\n",
    "|a|2|\n",
    "|sample|1|\n",
    "\n",
    "\n",
    "Document $d_2$\n",
    "\n",
    "|term|frequency|\n",
    "|----|---------|\n",
    "|this|1|\n",
    "|is|1|\n",
    "|another|2|\n",
    "|sample|3|\n",
    "\n",
    "\n",
    "If we want to understand the relevance of *this* in $d_1$ we can compute the tf and idf.\n",
    "\n",
    "$$tf(this,d_1)=\\dfrac{1}{5}$$\n",
    "\n",
    "The word *this* appears only one in $d_1$ from the total of $5$ terms (sum of all the frequencies).\n",
    "\n",
    "$$idf(this, D) = log\\dfrac{2}{2}=0$$\n",
    "\n",
    "With the idf we can get how relevant is *this* in the corpus $D$. It appears in both documents, therefore it does not bring too much information.\n",
    "\n",
    "If we compute the same for *another* we can see the word is more relevant as it appears with a higher frequency but not in all the documents of the corpus $D$.\n",
    "\n",
    "$$tf(another,d_2)=\\dfrac{2}{7}$$\n",
    "\n",
    "$$idf(another, D) = log\\dfrac{2}{1}=0.69$$\n",
    "\n",
    "We can combine both metrics using the **tfidf** to filter out common terms and highlight relevant terms:\n",
    "\n",
    "$$\n",
    "\\mathbf{tfidf}(t,d,D)=\\mathbf{tf}(t,d) \\cdot\\mathbf{idf}(t,D)\n",
    "$$\n",
    "\n",
    "With **tfidf** we can weight how relevant is a term inside a document considering how frequently it is found. For the previous example:\n",
    "\n",
    "$$\n",
    "\\mathbf{tfidf}(\\text{this},d1,D)=\\dfrac{1}{5} \\cdot 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{tfidf}(\\text{this},d1,D)=\\dfrac{1}{5} \\cdot 0\n",
    "$$\n",
    "\n",
    "In a practical scenario we will use vectorized versions. For a vocabulary {a,b,c} and three documents:\n",
    "\n",
    "|Input|TF-vector|\n",
    "|-----|---------|\n",
    "|{a,b,c} | {1,1,1} |\n",
    "|{a,b,c,c,a} | {2,2,1}|\n",
    "|{a,a,a,c,a} | {4,0,1}|\n",
    "\n",
    "Usually the vocabulary is trimmed by discarded terms with a small idf to avoid non-relevant terms. This can be done using the params `mindf` and `mintf` from the `CountVectorizer` transformer. Take a look at the [Bag of words](#Bag-of-words) section. \n",
    "\n",
    "The following example computes the idf of a corpus using the `IDF` transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28815db5-7970-49d5-8d31-14f87fa7f0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import IDF, Tokenizer, CountVectorizer\n",
    "\n",
    "corpus = session.createDataFrame ([\n",
    "                                (0.0, \"Java has been around for a while\"),\n",
    "                                (0.0, \"I wish Java could use case classes\"),\n",
    "                                (0.0, \"Objects belong to classes\")\n",
    "                                ], [\"label\", \"sentence\"])\n",
    "\n",
    "# First, get the tokens\n",
    "tokenizer = Tokenizer(inputCol ='sentence', outputCol ='tokens')\n",
    "tokens = tokenizer.transform(corpus)\n",
    "tokens.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e273c4a9-6ae3-4165-a57e-48355fa9a0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second, compute the frequency of every token. We could remove stop words here.\n",
    "# We are going to reduce the corpus to a vocabulary of two words for those tokens appearing at least twice in the corpus.\n",
    "vectorizer = CountVectorizer(inputCol='tokens', outputCol='frequencies', minDF=2.0, vocabSize=2)\n",
    "model = vectorizer.fit(tokens)\n",
    "print(\"This is our vocabulary %s\" % model.vocabulary)\n",
    "frequencies = model.transform(tokens)\n",
    "frequencies.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fea24e6-af7a-4fb4-9e48-261bcb15b71f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Third, we compute the IDF\n",
    "idf = IDF(inputCol='frequencies', outputCol='idf')\n",
    "idf_model = idf.fit(frequencies)\n",
    "idf_result = idf_model.transform(frequencies)\n",
    "idf_result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dffe00-a6d0-42d7-9fa5-22d21eed6c8f",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "The Word2Vec represents the words of a document in a vector. This makes possible to operate with documents as vectors which makes possible to easily computes distances and enables other algorithms specially in NLP. Take a look at the original Google code [here](https://code.google.com/archive/p/word2vec/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392260db-89c6-4d68-943a-daf85eb74b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "session = SparkSession.builder.appName('Example').master('local').getOrCreate()\n",
    "\n",
    "corpus = session.createDataFrame([\n",
    "    ('Spark is quite useful'.split(),),\n",
    "    ('I can use Spark with Python'.split(),),\n",
    "    ('Spark is not so difficult after all'.split(),),\n",
    "    ], ['words'])\n",
    "\n",
    "corpus.show(truncate=False)\n",
    "\n",
    "w2v = Word2Vec(inputCol='words', outputCol='result', vectorSize=3, minCount=0)\n",
    "w2v_model = w2v.fit(corpus)\n",
    "vectors = w2v_model.transform(corpus)\n",
    "vectors.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298b11a6-1209-406d-a4c3-9bbe460d05fa",
   "metadata": {},
   "source": [
    "# Pipelines\n",
    "\n",
    "Most models are computed as a concatenation of operations, each operation transforming the original dataset. For example, normalization -> component analysis -> regression. MLlib uses the concept of pipelines (similinar to the one used in [SciKit](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)) to unify the execution of a sequence of steps into a single object. \n",
    "\n",
    "The pipeline is defined as a sequence of stages connecting transformers and estimators:\n",
    "\n",
    "- **Transformer**: receives an input dataframe and returns a transformed version (standardizers)\n",
    "- **Estimator**: receives an input dataframe and after fitting returns a transformer (linear regression, logistic regression, etc.)\n",
    "\n",
    "Creating a pipeline is equivalent to set the sequence of stages to be executed.\n",
    "\n",
    "```python\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "pipeline = Pipeline(stages=[standardizer, pca, lr])\n",
    "```\n",
    "\n",
    "Then we fit the model and transform the dataset to get the corresponding results:\n",
    "\n",
    "```python\n",
    "model = pipeline.fit(dataset)\n",
    "model.transform(dataset).show()\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbec7aa-2b32-4bd3-a676-a34b5e8c154c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "\n",
    "session = SparkSession.builder.appName('Example').master('local').getOrCreate()\n",
    "\n",
    "corpus = session.createDataFrame([\n",
    "    ('Spark is quite useful',),\n",
    "    ('I can use Spark with Python',),\n",
    "    ('Spark is not so difficult after all',),\n",
    "    ], ['docs'])\n",
    "\n",
    "corpus.show(truncate=False)\n",
    "\n",
    "tokenizer = Tokenizer(inputCol='docs', outputCol='tokens')\n",
    "stop_remover = StopWordsRemover(inputCol='tokens', outputCol='filtered')\n",
    "cv = CountVectorizer(inputCol='filtered', outputCol='frequencies')\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, stop_remover, cv])\n",
    "\n",
    "fitted = pipeline.fit(corpus)\n",
    "result = fitted.transform(corpus)\n",
    "result.show(truncate=False)\n",
    "\n",
    "for m in fitted.stages:\n",
    "    print('-->',m.uid)\n",
    "    print(m.params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da88b32-14be-46a1-940a-9bfc61a43ced",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "The Spark MLlib offers a linear regression implementation with $L_1$, $L_2$ and ElasticNet regularization.\n",
    "\n",
    "```python\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(maxIter=10, regParam=0.3)\n",
    "    \n",
    "```\n",
    "\n",
    "In a naive example where we have a set of observatios corresponding to the equation $y=2x+3$:\n",
    "\n",
    "| y | x |\n",
    "|---|---|\n",
    "| 7 | 2 |\n",
    "| 9 | 3 |\n",
    "| 23 | 10 |\n",
    "\n",
    "We will train a model based on these observations to predict the output for $x=5$. Obviously, this will be $y=13$ ($y=2\\times 5 + 3$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f705518-2a54-4d99-938d-7ce357ae83ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "session = SparkSession.builder.appName(\"Example\").master(\"local\").getOrCreate()\n",
    "\n",
    "# our observations\n",
    "observations = [[7.0 , Vectors.dense ([2.0])] ,\n",
    "                [9.0 , Vectors.dense ([3.0])] ,\n",
    "                [23.0 , Vectors.dense ([10.0])]\n",
    "               ]\n",
    "\n",
    "# We create a dataset as a DataFrame using a colum for label, and a column for the features.\n",
    "# Observe that in this case, the features are a single column using a Vector.\n",
    "dataset = session.createDataFrame(observations, ['label', 'features'])\n",
    "\n",
    "dataset.show()\n",
    "\n",
    "print(\"Train our model...\")\n",
    "lr = LinearRegression()\n",
    "# Train the model\n",
    "model = lr.fit(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ef18b7-d5a6-4f73-91c5-0a011c4b244f",
   "metadata": {},
   "source": [
    "Now that we have trained our model we can investigate how does it look internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b724cf-91f1-4784-866f-39c528d8953d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show fitted model\n",
    "print(\"These are the weights for our model:\")\n",
    "print(\"Coefficients: %s\" % model.coefficients)\n",
    "print(\"Intercept: %s\" % model.intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484ad947-4419-4692-880b-8928f763129a",
   "metadata": {},
   "source": [
    "Unsurprisingly, we have a coefficient near 2 and the intercept value is 3. We were trying to model a linear function so it was a straight forward case.\n",
    "\n",
    "We can check how was the training in terms of error, iterations, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953b95f4-da76-4f66-aa2f-b62166ba8b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the training summary\n",
    "print(\"This is the training summary:\")\n",
    "print(\"Num iterations: %d\" % model.summary.totalIterations)\n",
    "print(\"Residuals:\")\n",
    "model.summary.residuals.show()\n",
    "print(\"RMSE: %f\" % model.summary.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d6abf3-f93d-4d9d-8de9-3cbbad08ec42",
   "metadata": {},
   "source": [
    "Now we are going to predict new outputs. We simply feed the model with dataframes using the same format we used to train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809d8426-a00a-4615-88a9-8473417a6de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this case we have the label and the observed feature value\n",
    "to_predict_items = [[13.0, Vectors.dense([5.0])]]\n",
    "\n",
    "to_predict = session.createDataFrame(to_predict_items, ['label', 'features'])\n",
    "\n",
    "predictions = model.transform(to_predict)\n",
    "predictions.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a97a75-b30d-4970-9d1c-e5165f33ddba",
   "metadata": {},
   "source": [
    "Because our model is basically perfect, we have a prediction of 13 which is what we were expecting.\n",
    "\n",
    "In a more complex scenario, we will need to know how good is our model doing. We can check the error using any of the available error evaluators. In this case, for a linear regression we can use the `RegressionEvaluator`. For our `predictions` this value will be 0 because we committed no mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561b68dd-6dc6-4441-afa6-57e0163c658f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "ev = RegressionEvaluator(metricName='rmse')\n",
    "\n",
    "ev.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae70415b-f481-420d-b247-ba6ff771fe8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a30a0e97-7dac-424a-9a04-538e86d2c8aa",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "The Spark MLLib offers a logicstic regression implementation for binominal and multinomial problems in the classification package (docs [here](https://spark.apache.org/docs/3.5.0/api/python/reference/api/pyspark.ml.classification.LogisticRegression.html?highlight=logistic#pyspark.ml.classification.LogisticRegression) for more details.)\n",
    "\n",
    "The example below trains a binary classifier to identify whether a point is contained inside a circle or not.\n",
    "\n",
    "| x | y | inside |\n",
    "|---|---|--------|\n",
    "|0 |0 | 1 |\n",
    "|0 |2 | 0 |\n",
    "|2 |0 | 0 |\n",
    "|1 |0 | 1 |\n",
    "|0 |1 | 1 |\n",
    "|0.3 | 0.87 | 1 |\n",
    "|1 | -1.3 | 1 |\n",
    "|0.9 | -1.2 | 1 |\n",
    "\n",
    "\n",
    "<img src=\"fig/log_circle.png\" width=\"300\"/>\n",
    "\n",
    "Similarly to what we did with the linear regression, we define our dataset based on the observations to fit our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cb366b9-e781-4837-abae-642649a7f815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|  features|\n",
      "+-----+----------+\n",
      "|  1.0| [0.0,0.0]|\n",
      "|  0.0| [0.0,2.0]|\n",
      "|  0.0| [2.0,0.0]|\n",
      "|  1.0| [1.0,0.0]|\n",
      "|  1.0| [0.0,1.0]|\n",
      "|  1.0|[0.3,0.87]|\n",
      "|  0.0|[1.0,-1.3]|\n",
      "|  1.0|[0.9,-1.2]|\n",
      "+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "session = SparkSession.builder.appName(\"Example\").master(\"local\").getOrCreate()\n",
    "\n",
    "observations = [[1.0, Vectors.dense([0.0 ,0.0])],\n",
    "                [0.0, Vectors.dense([0.0 ,2.0])],\n",
    "                [0.0, Vectors.dense([2.0 ,0.0])],\n",
    "                [1.0, Vectors.dense([1.0 ,0.0])],\n",
    "                [1.0, Vectors.dense([0.0 ,1.0])],\n",
    "                [1.0, Vectors.dense([0.3 ,0.87])],\n",
    "                [0.0, Vectors.dense([1.0, -1.3])],\n",
    "                [1.0, Vectors.dense([0.9, -1.2])]]\n",
    "\n",
    "dataset = session.createDataFrame(observations, ['label', 'features'])\n",
    "dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f6c60fd-3f4c-4387-8286-1c40bb135b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-2.6073306122669933,-1.002969191804841]\n",
      "Intercept: 2.380173815124733\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "lr = LogisticRegression()\n",
    "model = lr.fit(dataset)\n",
    "\n",
    "# Show model internals\n",
    "print(\"Coefficients: %s\" % model.coefficients)\n",
    "print(\"Intercept: %s\" % model.intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d88e7d-5659-4adc-99e8-01ac0954a2cc",
   "metadata": {},
   "source": [
    "We can take a look at the errors during the training phase. In this case, we look at the ROC curve, the AUC, and the F-measure by threshold.\n",
    "\n",
    "We can take a look at the ROC curve and the AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05096f73-2c3a-498b-a8ea-6c917537605b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---+\n",
      "|               FPR|TPR|\n",
      "+------------------+---+\n",
      "|               0.0|0.0|\n",
      "|               0.0|0.2|\n",
      "|               0.0|0.4|\n",
      "|               0.0|0.6|\n",
      "|0.3333333333333333|0.6|\n",
      "|0.3333333333333333|0.8|\n",
      "|0.6666666666666666|0.8|\n",
      "|0.6666666666666666|1.0|\n",
      "|               1.0|1.0|\n",
      "|               1.0|1.0|\n",
      "+------------------+---+\n",
      "\n",
      "AUC: 0.800000\n",
      "+--------------------+-------------------+\n",
      "|           threshold|          F-Measure|\n",
      "+--------------------+-------------------+\n",
      "|  0.9153029099975847|0.33333333333333337|\n",
      "|  0.7985416752560925| 0.5714285714285715|\n",
      "|  0.7750656745222227| 0.7499999999999999|\n",
      "|  0.7458695786407464| 0.6666666666666665|\n",
      "|  0.6737931410385649| 0.8000000000000002|\n",
      "|  0.5924820101887259| 0.7272727272727272|\n",
      "| 0.44345374176351793| 0.8333333333333333|\n",
      "|0.055488744288528125| 0.7692307692307693|\n",
      "+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ROC curve\n",
    "summary = model.summary\n",
    "summary.roc.show()\n",
    "\n",
    "# The AUC\n",
    "print(\"AUC: %f\" % summary.areaUnderROC)\n",
    "\n",
    "# F-measure by threshold\n",
    "summary.fMeasureByThreshold.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1276c26e-6efc-42dd-a858-ebe831386e7c",
   "metadata": {},
   "source": [
    "The ROC curve looks much nicer if we have a real curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85626cdc-dee5-48f4-b1f3-c424a5b31ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as pyplot\n",
    "\n",
    "# Transform it into something more handy\n",
    "fpr = summary.roc.select('FPR').rdd.map(lambda r: r[0]).collect()\n",
    "tpr = summary.roc.select('TPR').rdd.map(lambda r: r[0]).collect()\n",
    "\n",
    "pyplot.plot(fpr, tpr, 'b-')\n",
    "pyplot.xlabel('FPR')\n",
    "pyplot.ylabel('TPR')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db6b68b-1e0d-426d-a285-4ef3810da2e2",
   "metadata": {},
   "source": [
    "We do not have too many points to draw so our ROC curve looks so steeped.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9409835-7fb2-4367-afeb-b35b32ff3652",
   "metadata": {},
   "source": [
    "# Kmeans Clustering\n",
    "\n",
    "MLlib offers an implementation of KMeans with centroids initialization using [KMeans++]\n",
    "(https://en.wikipedia.org/wiki/K-means%2B%2B) ([docs](https://spark.apache.org/docs/3.5.0/api/python/reference/api/pyspark.ml.clustering.KMeans.html?highlight=kmeans#pyspark.ml.clustering.KMeans))\n",
    "\n",
    "The following example runs KMeans with $k=2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba7ffa38-6699-4c9f-971c-441f4e148953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|  features|\n",
      "+----------+\n",
      "| [0.0,0.0]|\n",
      "| [0.0,2.0]|\n",
      "| [2.0,0.0]|\n",
      "| [1.0,0.0]|\n",
      "| [0.0,1.0]|\n",
      "|[0.3,0.87]|\n",
      "|[1.0,-1.3]|\n",
      "|[0.9,-1.2]|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "echo \"\n",
    "x,y\n",
    "0,0\n",
    "0,2\n",
    "2,0\n",
    "1,0\n",
    "0,1\n",
    "0.3,0.87\n",
    "1,-1.3\n",
    "0.9,-1.2\" > /tmp/kmeans_example.csv\n",
    "'''\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "session = SparkSession.builder.appName(\"Example\").master(\"local\").getOrCreate()\n",
    "\n",
    "# Create the dataset\n",
    "observations = [[Vectors.dense([0,0])],\n",
    "                [Vectors.dense([0,2])],\n",
    "                [Vectors.dense([2,0])],\n",
    "                [Vectors.dense([1,0])],\n",
    "                [Vectors.dense([0,1])],\n",
    "                [Vectors.dense([0.3,0.87])],\n",
    "                [Vectors.dense([1,-1.3])],\n",
    "                [Vectors.dense([0.9,-1.2])]\n",
    "               ]\n",
    "\n",
    "dataset = session.createDataFrame(observations, ['features'])\n",
    "dataset.show()\n",
    "\n",
    "# Configure and fit the model\n",
    "kmeans = KMeans(k=2)\n",
    "model = kmeans.fit(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898588b7-a860-4ea8-ba14-8105033ceb85",
   "metadata": {},
   "source": [
    "Once the model is fit, we can take a look at the predicted clusters and the generated centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "181b82e3-6f71-49a8-b5c8-be3bd0c4de4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centroids [array([0.075 , 0.9675]), array([ 1.225, -0.625])]\n"
     ]
    }
   ],
   "source": [
    "print(\"Centroids %s\" % model.clusterCenters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ceade20d-be89-44f6-bb97-9873240c8bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|  features|prediction|\n",
      "+----------+----------+\n",
      "| [0.0,0.0]|         0|\n",
      "| [0.0,2.0]|         0|\n",
      "| [2.0,0.0]|         1|\n",
      "| [1.0,0.0]|         1|\n",
      "| [0.0,1.0]|         0|\n",
      "|[0.3,0.87]|         0|\n",
      "|[1.0,-1.3]|         1|\n",
      "|[0.9,-1.2]|         1|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.summary.predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b326a8-be0d-447c-a184-af8afb7643c3",
   "metadata": {},
   "source": [
    "For a more visual analysis, we can plot the centroids and the reference circle. There is not too much data for a good clustering. However, the centroids are located inside and outside the circle which in a first attempt looks promising. Obviously the data distribution is a bit biased and forces one of the centroids to the bottom right. How the algorithm would evolve with a more uniformly distributed dataset remains as an additional exercise.\n",
    "\n",
    "Observe that we need some workaround to get the correct `numpy` shape for our plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8e7b11d-17f6-4af1-b4aa-7955d3bbb040",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as pyplot\n",
    "import numpy as np\n",
    "\n",
    "# Transform it into something more handy\n",
    "features = model.summary.predictions.select('features').rdd.map(lambda r: r[0]).collect()\n",
    "predictions = model.summary.predictions.select('prediction').rdd.map(lambda r: r[0]).collect()\n",
    "\n",
    "x,y = np.array(features)[:,0], np.array(features)[:,1]\n",
    "\n",
    "ax = pyplot.gca()\n",
    "ax.cla()\n",
    "\n",
    "# Plot the predictions\n",
    "ax.scatter(x,y,c=predictions)\n",
    "\n",
    "# Plot the centroids\n",
    "centroids = np.array(model.clusterCenters())\n",
    "ax.scatter(x=centroids[:,0], y=centroids[:,1], marker='x')\n",
    "\n",
    "# Plot the circle as a reference\n",
    "circle = pyplot.Circle((0,0),1, color='blue', fill=False)\n",
    "ax.add_patch(circle)\n",
    "\n",
    "ax.set_xlim((-2.5,2.5))\n",
    "ax.set_ylim((-2.5,2.5))\n",
    "\n",
    "# Make a square plot for a better visualization of the circle\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "pyplot.show()\n",
    "\n",
    "session.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7386f590-7665-4e2f-b7eb-1422fb827147",
   "metadata": {},
   "source": [
    "# Topic Modelling\n",
    "\n",
    "The Latent Dirichlet Allocation (LDA) (do not confuse with Linear Discriminant Analysis) is a method for topic modelling. This method can be used to extract the topics contained in a corpus. For example: extract the topics from a collection of users' opinions, main ideas discussed in wiki-leaks, etc.\n",
    "\n",
    "LDA represents documents as a mixture that spit out words with certain probabilities. The final model is solved using bayesian inference.\n",
    "\n",
    "$$\n",
    "P(\\boldsymbol{W}, \\boldsymbol{Z}, \\boldsymbol{\\theta}, \\boldsymbol{\\varphi};\\alpha,\\beta) = \\prod_{i=1}^K P(\\varphi_i;\\beta) \\prod_{j=1}^M P(\\theta_j;\\alpha) \\prod_{t=1}^N P(Z_{j,t}\\mid\\theta_j)P(W_{j,t}\\mid\\varphi_{Z_{j,t}})\n",
    "$$\n",
    "\n",
    "where\n",
    "- $\\alpha$ is the parameter of the Dirichlet prior on the per-document topic distributions\n",
    "- $\\beta$ is the parameter of the Dirichlet prior on the per-topic word distribution\n",
    "- $\\theta_{m}$ is the topic distribution for document m\n",
    "- $\\varphi_{k}$ is the word distribution for topic k\n",
    "- $z_{mn}$ is the topic for the n-th word in document m\n",
    "- $w_{mn}$ is the specified word\n",
    "\n",
    "In pseudocode the algorithm can be described as follows [[Blei et al.](http://ai.stanford.edu/~ang/papers/jair03-lda.pdf)]:\n",
    "\n",
    "    Assign randomly each word in every document a topic\n",
    "    for number of iterations do\n",
    "        for every word w in document d do\n",
    "            for each topic t ∈ K do\n",
    "                Compute the proportion of words in d assigned to t\n",
    "                Compute the proportion of assignments to t that come from w\n",
    "                Reassign a new topic using p(t|d) · p(w|t)\n",
    "            end for\n",
    "        end for\n",
    "    end for\n",
    "\n",
    "After a certain number of iterations, LDA reaches a steady state with a mixture of topics per document. Every document belongs to the $K$ topics with a certain probability. A plausible example could be as follows:\n",
    "\n",
    "| Document | Probability topic 1 | Probability topic 2 |\n",
    "|----------|---------------------|---------------------|\n",
    "|I like apples | 1 | 0 |\n",
    "|I eat apples | 0  | 1 |\n",
    "|There is no white apples | 0.8 | 0.2 |\n",
    "|I don't eat apples | 0.3 | 0.7 |\n",
    "\n",
    "Spark MLlib offers and LDA implementation using:\n",
    "- EMLDAOptimizer: batch approach\n",
    "- OnlineLDAOptimizer: incremental learning solution\n",
    "\n",
    "The fitted model offers a description of the $K$ topics and the transformer computes the probability of every document to belong to every topic as described above ([ref](https://spark.apache.org/docs/3.5.0/api/python/reference/api/pyspark.ml.clustering.LDA.html?highlight=lda#lda))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "683f6e47-e4aa-4bb7-abfd-3b9cba8a9d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------------------------+------------------------------------------+\n",
      "|label|sentence                          |tokens                                    |\n",
      "+-----+----------------------------------+------------------------------------------+\n",
      "|0.0  |Java has been around for a while  |[java, has, been, around, for, a, while]  |\n",
      "|0.0  |I wish Java could use case classes|[i, wish, java, could, use, case, classes]|\n",
      "|0.0  |Objects belong to classes in Java |[objects, belong, to, classes, in, java]  |\n",
      "+-----+----------------------------------+------------------------------------------+\n",
      "\n",
      "This is our vocabulary ['java', 'classes']\n",
      "+-----+--------------------+--------------------+-------------------+\n",
      "|label|            sentence|              tokens|        frequencies|\n",
      "+-----+--------------------+--------------------+-------------------+\n",
      "|  0.0|Java has been aro...|[java, has, been,...|      (2,[0],[1.0])|\n",
      "|  0.0|I wish Java could...|[i, wish, java, c...|(2,[0,1],[1.0,1.0])|\n",
      "|  0.0|Objects belong to...|[objects, belong,...|(2,[0,1],[1.0,1.0])|\n",
      "+-----+--------------------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import IDF, Tokenizer, CountVectorizer\n",
    "\n",
    "session = SparkSession.builder.appName(\"Example\").master(\"local\").getOrCreate()\n",
    "\n",
    "corpus = session.createDataFrame ([\n",
    "                                (0.0, 'Java has been around for a while'),\n",
    "                                (0.0, 'I wish Java could use case classes'),\n",
    "                                (0.0, 'Objects belong to classes in Java'),\n",
    "                                ], ['label', 'sentence'])\n",
    "\n",
    "# First, get the tokens\n",
    "tokenizer = Tokenizer(inputCol ='sentence', outputCol ='tokens')\n",
    "tokens = tokenizer.transform(corpus)\n",
    "tokens.show(truncate=False)\n",
    "\n",
    "\n",
    "# Second, compute the frequency of every token. We could remove stop words here.\n",
    "# We are going to reduce the corpus to a vocabulary of two words for those tokens appearing at least twice in the corpus.\n",
    "vectorizer = CountVectorizer(inputCol='tokens', outputCol='frequencies', minDF=2.0, vocabSize=2)\n",
    "model = vectorizer.fit(tokens)\n",
    "print(\"This is our vocabulary %s\" % model.vocabulary)\n",
    "frequencies = model.transform(tokens)\n",
    "frequencies.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4e7dc70-5cf6-496f-9f29-48fd176fd2ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+----------------------------------------+\n",
      "|topic|termIndices|termWeights                             |\n",
      "+-----+-----------+----------------------------------------+\n",
      "|0    |[0, 1]     |[0.6384828885791629, 0.3615171114208371]|\n",
      "|1    |[0, 1]     |[0.5614431009841063, 0.4385568990158936]|\n",
      "+-----+-----------+----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import LDA\n",
    "lda = LDA(k=2, maxIter = 10, featuresCol='frequencies', optimizer='em')\n",
    "lda_model = lda.fit(frequencies)\n",
    "\n",
    "topics = lda_model.describeTopics(3)\n",
    "topics.show(truncate=False)\n",
    "\n",
    "session.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec02c46-bd2e-49f3-a0ee-6ee9c5b043eb",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "In order to understand the correctness of a model we need to compute its associated prediction error. First, we have to understand that during the training phase (fitting) our model should not use observations used during the evaluation phase. In this way, we check if our model can really make predictions on an observation never seen before and avoid potential overfitting. To do so, we have to differentiate between training and test datasets.\n",
    "\n",
    "For the evaluation, MLlib offers a collection of evaluators. Depending on the prediction problem we are working on, we have to select the corresponding error evaluator which usually supports a variety of metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e60395b-da1c-43b6-bac2-71149f1acaaa",
   "metadata": {},
   "source": [
    "## Random split\n",
    "\n",
    "A simple way to obtain is using the `randomSplit` method from the data frame objects. We simply indicate the ratio from the total of rows dedicated to the train and evaluation and we get both "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53891e42-33b9-4f75-b092-b534ed11319a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "session = SparkSession.builder.appName('Example').master('local').getOrCreate()\n",
    "\n",
    "dataset = session.createDataFrame([\n",
    "    (0, \"I think I saw something\"),\n",
    "    (1, \"PySpark is awesome\"),\n",
    "    (2, \"Python = data powers\"),\n",
    "    (3, \"You don't need scala to use Spark\"),\n",
    "    (4, \"Programming is an art\"),\n",
    "], ['id', 'doc'])\n",
    "\n",
    "dataset.show()\n",
    "\n",
    "train_ratio = 0.6\n",
    "eval_ratio = 0.4\n",
    "\n",
    "train, eval = dataset.randomSplit([train_ratio, eval_ratio])\n",
    "print('Our training dataset')\n",
    "train.show(truncate=False)\n",
    "print('Our evaluation dataset')\n",
    "eval.show(truncate=False)\n",
    "\n",
    "session.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1538d8-d97a-49e9-b18f-bed40f07e9a0",
   "metadata": {},
   "source": [
    "## Binary classification\n",
    "\n",
    "The `BinaryClassificationEvaluator` expects a column with the labels and one with the predictions. The predictions can be represented in two ways:\n",
    "- Type double. Where we represent binary values 0/1 or the probability of having label 1.\n",
    "- Length 2 vector. Raw predictions, scores, or label probabilities.\n",
    "\n",
    "Two different metrics can be selected with the `metricName` param: `areaUnderPR` or `areaUnderROC`. Check the [docs](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.mllib.evaluation.BinaryClassificationMetrics.html?highlight=areaunderpr#binaryclassificationmetrics) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3044afe8-a92a-4fa1-b6e8-83931c72f31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "session = SparkSession.builder.appName('Example').master('local').getOrCreate()\n",
    "\n",
    "label_preds = session.createDataFrame([\n",
    "    (1.0, 0.35),\n",
    "    (1.0, 0.9),\n",
    "    (1.0, 0.18),\n",
    "    (0.0, 1.0),\n",
    "],['label', 'prediction'])\n",
    "\n",
    "label_preds.show()\n",
    "\n",
    "bce = BinaryClassificationEvaluator(labelCol='label', rawPredictionCol='prediction', metricName='areaUnderPR')\n",
    "bce.evaluate(label_preds)\n",
    "\n",
    "session.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3916e1f-146f-4df1-9b6f-a17c93faa465",
   "metadata": {},
   "source": [
    "## Regression \n",
    "\n",
    "Evaluator for regression models. The following metrics can be specified in the `metricName` param:\n",
    "- rmse: root mean squared error (default)\n",
    "- mse: mean squared error\n",
    "- r2: $r^2$ metric\n",
    "- mae: mean absolute error\n",
    "- var: explained variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642434d4-e827-4f74-b949-a66fadbe24d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "session = SparkSession.builder.appName('Example').master('local').getOrCreate()\n",
    "\n",
    "label_preds = session.createDataFrame([\n",
    "    (15.0, 11.0),\n",
    "    (5.32, 6.0),\n",
    "    (1.40, 3.8),\n",
    "    (5.0, 1.8),\n",
    "],['label', 'prediction'])\n",
    "\n",
    "label_preds.show()\n",
    "\n",
    "# We compute the mean absolute error (MAE)\n",
    "re = RegressionEvaluator(labelCol='label', predictionCol='prediction', metricName='mae')\n",
    "re.evaluate(label_preds)\n",
    "\n",
    "session.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd426e9-146d-4756-9cca-87d286b31340",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "\n",
    "Cross validation splits the dataset into $k$ folds and uses $k-1$ folds for training and one fold for evaluation. This operaiton is done $k$ times ensuring variability during the process.\n",
    "\n",
    "Assuming $k=4$ we split a dataset $D$ into $\\{D_0,D_1,D_2,D_3\\}$ generating the training-evaluation pairs as shown below.\n",
    "\n",
    "| Training | Evaluation |\n",
    "|:----------:|------------:|\n",
    "| $$\\{D_1,D_2,D_3\\}$$ | $\\{D_0\\}$ |\n",
    "| $$\\{D_0,D_2,D_3\\}$$ | $\\{D_1\\}$ |\n",
    "| $$\\{D_0,D_1,D_3\\}$$ | $\\{D_2\\}$ |\n",
    "| $$\\{D_0,D_1,D_2\\}$$ | $\\{D_3\\}$ |\n",
    "\n",
    "\n",
    "\n",
    "The cross validation has several advantages:\n",
    "- It is [embarrasingly parallel](https://en.wikipedia.org/wiki/Embarrasissingly_parallel)\n",
    "- The quality of a model is the average of its evaluation over the different evaluatio sets\n",
    "- Userful approach to the hyperparameter problem\n",
    "\n",
    "And some disadvantages:\n",
    "- It can be very expensive with large datasets\n",
    "- Transformations using information extracted from the whole dataset instead of the training folds may lead to biased results\n",
    "\n",
    "In MLlib we simply have to configure a `CrossValidator` indicating the estimator, the evaluator, the params (if any) and the number of folds.\n",
    "\n",
    "```python\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "cv = CrossValidator(estimator=pipeline, evaluator=BinaryClassificationEvaluator(), estimatorParamMaps=paramGrid, numFolds=10)\n",
    "```\n",
    "\n",
    "The `CrossValidator` will use the evaluator to identify the best model. Then, it can be used as usual\n",
    "\n",
    "```python\n",
    "cv_model = cv.fit(dataset)\n",
    "cv_model.transform(testDataset)\n",
    "```\n",
    "\n",
    "The `ParamGridBuilder` permits to create a grid for the search of hyperparameters that can be later used in the cross validation.\n",
    "\n",
    "```python\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "grid = ParamGridBuilder()\n",
    "grid.addGrid(param=lm.elasticNetParam, values=[0.1,0.3])\n",
    "grid.build()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
